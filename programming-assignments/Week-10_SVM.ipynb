{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1BL6T_E7BVMU2koexzDznLjcn-ngWANdr","timestamp":1669191317195}],"authorship_tag":"ABX9TyMtVP2lDj9MtkSd+hWphok5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Support Vector Machines\n","\n","We wil implement both hard-margin SVMs and soft-margin SVMs from scratch on a toy dataset. Apart from `NumPy`, we would need to take the help of `SciPy` for solving the quadratic programming problem."],"metadata":{"id":"Kwt8A5yBXwAW"}},{"cell_type":"markdown","source":["## Hard-Margin SVM"],"metadata":{"id":"49WrewU6X1aR"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","plt.rcParams['figure.figsize'] = [12, 12]"],"metadata":{"id":"1bIDOxRZWRP9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### DATA: DO NOT EDIT THIS CELL ####\n","X = np.array([[1, -3], [1, 0], [4, 1], [3, 7], [0, -2],\n","             [-1, -6], [2, 5], [1, 2], [0, -1], [-1, -4],\n","             [0, 7], [1, 5], [-4, 4], [2, 9], [-2, 2],\n","             [-2, 0], [-3, -2], [-2, -4], [3, 10], [-3, -8]]).T\n","y = np.array([1, 1, 1, 1, 1, \n","             1, 1, 1, 1, 1,\n","             -1, -1, -1, -1, -1, \n","             -1, -1, -1, -1, -1])"],"metadata":{"id":"CO6QjhLnWYX3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Problem-1\n","\n","$\\mathbf{X}$ is a data-matrix of shape $(d, n)$. $\\mathbf{y}$ is a vector of labels of size $(n, )$. What is the value of $n$ and $d$?"],"metadata":{"id":"nhWvMF3QYQdx"}},{"cell_type":"code","source":["### Solution ###"],"metadata":{"id":"EOO0y6UaapAa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Problem-2\n","\n","Visualize the dataset given to you using a scatter plot. Colour points which belong to class $+1$ $\\color{green}{\\text{green}}$ and those that belong to $-1$ $\\color{red}{\\text{red}}$. Inspect the data visually and determine its linear separability."],"metadata":{"id":"aI0L-xJ4aj-d"}},{"cell_type":"code","source":["### Solution ###"],"metadata":{"id":"kbjQsLMSXuDc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Problem-3\n","\n","Compute the object $\\mathbf{Y}$ that appears in the dual problem. What kind of an object is $\\mathbf{Y}$?"],"metadata":{"id":"uDOb9EmzZwWG"}},{"cell_type":"code","source":["### Solution ###"],"metadata":{"id":"EzgoThG4aPej"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Problem-4\n","\n","Let $\\boldsymbol{\\alpha}$ be the dual variable. The dual objective is of the form:\n","\n","$$\n","f(\\boldsymbol{\\alpha}) = \\boldsymbol{\\alpha}^T \\mathbf{1} - \\cfrac{1}{2} \\cdot \\boldsymbol{\\alpha}^T \\mathbf{Q} \\boldsymbol{\\alpha}\n","$$\n","\n","\n","\n","Compute the matrix $\\mathbf{Q}$ for this problem and find the sum of its elements. What properties does the matrix $\\mathbf{Q}$ have? What is the nature of the objective function?"],"metadata":{"id":"Zbbv04-qa_c3"}},{"cell_type":"code","source":["### Solution ###"],"metadata":{"id":"p_AsPP_nb48r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Problem-5\n","\n","Since `SciPy`'s optimization routines take the form of minimizing a function, we will recast $f$ as follows:\n","\n","$$\n","f(\\boldsymbol{\\alpha}) =  \\cfrac{1}{2} \\cdot \\boldsymbol{\\alpha}^T \\mathbf{Q} \\boldsymbol{\\alpha} - \\boldsymbol{\\alpha}^T \\mathbf{1}\n","$$\n","\n","We now have to solve :\n","\n","$$\n","\\min \\limits_{\\boldsymbol{\\alpha} \\geq 0} \\quad f(\\boldsymbol{\\alpha})\n","$$\n","\n","Note that $\\max$ changes to $\\min$ since we changed the sign of the objective function.\n","\n","<hr>\n","\n","Write a function `loss` that returns the value of objective function $f(\\boldsymbol{\\alpha})$ for argument $\\boldsymbol{\\alpha}$. Compute the value of `loss` at $\\boldsymbol{\\alpha} = \\mathbf{1}$.\n","\n","**Note**: The reason for naming the function `loss` is that we will be using `SciPy`'s `scipy.optimize.minize` routine."],"metadata":{"id":"O4QlhIsWyxYe"}},{"cell_type":"code","source":["### Solution ###"],"metadata":{"id":"TXGgoTZ4z63Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Problem-6\n","\n","Write a function named `jac` that computes the gradient, $\\nabla f(\\boldsymbol{\\alpha})$, given $\\boldsymbol{\\alpha}$ as argument. Compute the value of `jac` at $\\boldsymbol{\\alpha} = \\mathbf{1}$ and print the sum of the components of the gradient vector.\n","\n","**Note**: `jac` stands for Jacobian. In our case, we don't have a vector valued function. So, this will just be the gradient."],"metadata":{"id":"N0ksPS8a0N6P"}},{"cell_type":"code","source":["### Solution ###"],"metadata":{"id":"CEYzryQn3xRZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Problem-7\n","\n","Finally, we have most of the ingredients to solve the dual problem:\n","\n","$$\n","\\min \\limits_{\\boldsymbol{\\alpha} \\geq 0} \\quad \\cfrac{1}{2} \\cdot \\boldsymbol{\\alpha}^T \\mathbf{Q} \\boldsymbol{\\alpha} - \\boldsymbol{\\alpha}^T \\mathbf{1}\n","$$\n","\n","Go through this [document](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) to understand how `scipy.optimize.minimize` works. Few pointers:\n","\n","(1)  You should pass five arguments to `scipy.optimize.minimize`: `loss`, `jac`, `alpha_init`, `method`, `Bounds`\n","\n","(2) Use the method `SLSQP`. You can treat this as a black-box.\n","\n","(3) Set the initial value of `alpha_init` to zero.\n","\n","(4) Use `scipy.optimize.Bounds` to trigger the $\\boldsymbol{\\alpha} \\geq 0$ constraint.\n","\n","Compute the sum of components of the optimal solution, $\\boldsymbol{\\alpha}^*$. Enter the nearest integer as your answer."],"metadata":{"id":"Plv87Si05V5G"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"y_Wcmwy3grQ7"},"outputs":[],"source":["### Solution ###"]},{"cell_type":"markdown","source":["### Problem-8\n","\n","Find all the support vectors. Print the indices (zero-indexing) in the data-matrix where these support vectors are found."],"metadata":{"id":"hrruRySrPPJL"}},{"cell_type":"code","source":["### Solution ###"],"metadata":{"id":"d7JTzQniPZsg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Problem-9\n","\n","Find the optimal weight vector $\\mathbf{w}^*$. Round each component of the optimal weight vector to the nearest integer."],"metadata":{"id":"VA4rcEt2NwCI"}},{"cell_type":"code","source":["### Solution ###"],"metadata":{"id":"WbIi8Snu6Qga"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Problem-10\n","\n","Plot the decision boundary along with the supporting hyperplanes. Note where the support vectors lie in this plot. How many red points lie on the supporting hyperplanes? How many green points lie on the supporting hyperplanes?"],"metadata":{"id":"1pRJINcgRAWs"}},{"cell_type":"code","source":["### Solution ###"],"metadata":{"id":"2YiYiZyZbRUK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Soft-margin SVM\n","\n","We now turn to soft-margin SVMs. Adapt the hard-margin code that you have written for the soft-margin problem. The only change you have to make is to introduce an upper bound for $\\boldsymbol{\\alpha}$, which is the hyperparameter $C$.\n"],"metadata":{"id":"dsM9fbm_R-Vb"}},{"cell_type":"code","source":["### Solution ###"],"metadata":{"id":"OXSn3CvMS-iT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Problem-11\n","\n","Plot the decision boundary and the supporting hyperplane for the following values of $C$.\n","\n","(1) $C = 0.01$\n","\n","(2) $C = 0.1$\n","\n","(3) $C = 1$\n","\n","(4) $C = 10$\n","\n","Plot all of them in a $2 \\times 2$ subplot. Study the tradeoff between the following quantities:\n","\n","(1) Width of the margin.\n","\n","(2) Number of points that lie within the margin or on the wrong side. This is often called **margin violation**.\n"],"metadata":{"id":"1fngF_sW-jKD"}},{"cell_type":"code","source":["### Solution ###"],"metadata":{"id":"vVKn4mZfTzJu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Problem-12\n","\n","For $C = 10$, how many support vectors does the model have? Observe where these points lie. Does this observation tally with your understanding of the last few minutes of the soft-SVM lecture where professor summarizes the relationship between $\\alpha^{*}$ and the supporting hyperplanes?"],"metadata":{"id":"LAHR_IChAq4f"}},{"cell_type":"code","source":["### Solution ###"],"metadata":{"id":"HAnccH_-CRqA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Problem-13\n","\n","For $C = 10$, find the most critical support vectors, that is, those points for which $\\alpha^{*}_i = C$."],"metadata":{"id":"sTOeUowFAvUD"}},{"cell_type":"code","source":["### Solution ###"],"metadata":{"id":"8Xlc1rAF_m-g"},"execution_count":null,"outputs":[]}]}